{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Feature Engineering ", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "Import the necessary libraries/modules:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nimport librosa\nimport librosa.display\nimport os\nfrom os.path import isdir, join\nimport csv\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport random\nimport soundfile as sf\nfrom scipy.fftpack import fft\nfrom scipy import signal\nfrom scipy.io import wavfile\nimport webrtcvad\nimport IPython.display as ipd\nfrom scipy import stats"
        }, 
        {
            "execution_count": 54, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from ibm_botocore.client import Config\nimport ibm_boto3\ncos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials_1['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials_1['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials_1['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials_1['ENDPOINT'])"
        }, 
        {
            "source": "Set up variables for labels, directories and the supplied validation and training lists to split dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls data/train/audio"
        }, 
        {
            "execution_count": 48, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'eight', 'one', 'silence', 'happy', 'cat', 'sheila', 'three', 'nine', 'two', 'marvin', 'house', 'dog', 'bird', 'bed', 'tree', 'seven', 'wow', 'six', 'five', 'four', 'zero']\n"
                }
            ], 
            "source": "classes = ['yes', 'no', \n           'up', 'down', \n           'left', 'right', \n           'on', 'off', \n           'stop', 'go', \n           'unknown']\n\nfolders = os.listdir('./data/processed/')\n# put folders in same order as in the classes list, used when making sets\nall_classes = [x for x in classes[:11]]\nfor ind, cl in enumerate(folders):\n    if cl not in classes:\n        all_classes.append(cl)\nprint(all_classes)"
        }, 
        {
            "execution_count": 46, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "processed_audio_path = 'data/processed/'\ntrain_audio_path = 'data/train/audio/'\ndirs = [d for d in os.listdir(train_audio_path) if d in all_classes]"
        }, 
        {
            "execution_count": 49, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with open('data/train/validation_list.txt') as val_list:\n    validation_list = [row[0] for row in csv.reader(val_list)]\nassert len(validation_list) == 6798, 'file not loaded'\n\n#add silence files to validation_list\nfor i, file in enumerate(os.listdir(processed_audio_path + 'silence/')):\n    if i%10==0:\n        validation_list.append('silence/'+file)\n\ntraining_list = []\nall_files_list = []\nclass_counts = {}\n\nfor direct in dirs:\n    files = os.listdir(processed_audio_path + direct)\n    for i, f in enumerate(files):\n        all_files_list.append(direct + '/' + f)\n        path = direct + '/' + f\n        if path not in validation_list:\n            training_list.append(direct + '/' + f)        \n        class_counts[direct] = i\n\n#remove filenames from validation_list that don't exist anymore (due to eda)\nvalidation_list = list(set(validation_list).intersection(all_files_list))"
        }, 
        {
            "source": "## Feature Creation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In order for our models to process the data we have to extract some information from the wav files. The two best ways to do this that I found were using Spectrograms (essentially a STFT of the signal) and FFTs of the signal. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Spectrogram\nFor spectrograms, mel-power, log-scaled spectrograms yielded the best results. Raw log-specs of the signal were also tested but performed more poorly.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def make_spec(file, file_dir = processed_audio_path, flip = False, ps = False, st = 4):\n    \"\"\"\n    create a melspectrogram from the amplitude of the sound\n    \n    Args:\n        file (str): filename\n        file_dir (str): directory path\n        flip (bool): reverse time axis\n        ps (bool): pitch shift\n        st (int): half-note steps for pitch shift\n    Returns:\n        np.array with shape (122,85) (time, freq)\n    \"\"\"\n    sig, rate = sf.read(file_dir + file)\n    if len(sig) < 16000: # pad shorter than 1 sec audio with ramp to zero\n        sig = np.pad(sig, (0,16000-len(sig)), 'linear_ramp')\n    if ps:\n        sig = librosa.effects.pitch_shift(sig, rate, st)\n    D = librosa.amplitude_to_db(librosa.stft(sig[:16000], n_fft = 512, \n                                             hop_length = 128, \n                                             center = False), ref = np.max)\n    S = librosa.feature.melspectrogram(S=D, n_mels = 85).T\n    if flip:\n        S = np.flipud(S)\n    return S.astype(np.float32)"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'''def make_logspec(file, file_dir = processed_audio_path, flip = False, ps = False, st = 4):\n   \n    sig, rate = sf.read(file_dir + file)\n    if len(sig) < 16000: # pad shorter than 1 sec audio with ramp to zero\n        sig = np.pad(sig, (0,16000-len(sig)), 'linear_ramp')\n    if ps:\n        sig = librosa.effects.pitch_shift(sig, rate, st)\n    D = librosa.amplitude_to_db(librosa.stft(sig[:16000], n_fft = 512, \n                                             hop_length = 128, \n                                             center = False), ref = np.max).T\n    #S = librosa.feature.melspectrogram(S=D, n_mels = 85).T\n    if flip:\n        S = np.flipud(S)\n    return D.astype(np.float32)'''"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def create_sets(file_list = training_list):\n    X_array = np.zeros([len(file_list),122,85])\n    Y_array = np.zeros([len(file_list)])    \n    for ind, file in enumerate(file_list):\n        if ind%2000 == 0:\n            print(ind, file)    \n        try:\n            X_array[ind] = make_spec(file)\n        except (ValueError):\n            print(ind, file, ValueError)\n        Y_array[ind] = classes.index(file.rsplit('/')[0])\n        \n    return X_array, Y_array   "
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'''def create_sets_log(file_list = training_list):\n    X_array = np.zeros([len(file_list),122,257])\n    Y_array = np.zeros([len(file_list)])    \n    for ind, file in enumerate(file_list):\n        if ind%2000 == 0:\n            print(ind, file) \n\n       # try:    \n        X_array[ind] = make_logspec(file)\n       # except ValueError:\n           # print(ind, file, ValueError)\n        Y_array[ind] = all_classes.index(file.rsplit('/')[0])\n        \n    return X_array, Y_array  '''"
        }, 
        {
            "source": "Create and save the training and validation sets of spectrograms:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0 no/8830e17f_nohash_1.wav\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/gpfs/fs01/user/sce8-5e3b8351a3eeda-ba755807525d/.local/lib/python3.5/site-packages/librosa/core/spectrum.py:960: UserWarning: amplitude_to_db was called on complex input so phase information will be discarded. To suppress this warning, call amplitude_to_db(np.abs(S)) instead.\n  warnings.warn('amplitude_to_db was called on complex input so phase '\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "2000 no/28497c5b_nohash_1.wav\n4000 eight/6af4aa07_nohash_1.wav\n6000 one/4c4d2526_nohash_1.wav\n8000 up/0137b3f4_nohash_4.wav\n10000 happy/48a8a69d_nohash_0.wav\n12000 off/f8f60f59_nohash_4.wav\n14000 on/1a673010_nohash_0.wav\n16000 cat/8f811bbc_nohash_0.wav\n18000 stop/4c77947d_nohash_1.wav\n20000 three/3389305e_nohash_0.wav\n22000 yes/190821dc_nohash_2.wav\n24000 yes/61d3e51e_nohash_0.wav\n26000 nine/3bc21161_nohash_2.wav\n28000 down/23abe1c9_nohash_0.wav\n30000 two/14587ff0_nohash_0.wav\n32000 marvin/a84dee7b_nohash_1.wav\n34000 dog/d103dd6e_nohash_0.wav\n38000 bed/f174517e_nohash_1.wav\n40000 left/c71e3acc_nohash_0.wav\n42000 right/2da58b32_nohash_2.wav\n44000 right/652b3da7_nohash_0.wav\n46000 seven/e53139ad_nohash_3.wav\n48000 six/f4386675_nohash_1.wav\n50000 five/caf9fceb_nohash_0.wav\n52000 four/66276b0e_nohash_1.wav\n54000 four/66aa0f29_nohash_0.wav\n56000 zero/aba19127_nohash_1.wav\n58000 go/f19c1390_nohash_1.wav\n"
                }
            ], 
            "source": "X_train, Y_train_all = create_sets()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Y_train = np.where(Y_train_all < 11, Y_train_all, 11)"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 27, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(58299, 122, 257)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "X_train.shape"
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nnp.save('data/X_train_log2.npy', np.expand_dims(X_train, -1)+1.3)\nnp.save('data/Y_train_log.npy', Y_train.astype(np.int))\nnp.save('data/Y_train_all_log.npy', Y_train_all.astype(np.int))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nnp.save('data/X_train_mfcc_norm.npy', np.expand_dims(X_train_mfcc, -1)+1.3)\nnp.save('data/Y_train_mfcc_norm.npy', Y_train_mfcc_sorted.astype(np.int))\nnp.save('data/Y_train_mfcc_all_norm.npy', Y_train_mfcc.astype(np.int))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_val, Y_val_all = create_sets(file_list = validation_list)"
        }, 
        {
            "execution_count": 40, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Y_val_log = np.where(Y_val_log_all < 11, Y_val_log_all, 11)"
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "np.save('data/X_val_log.npy', np.expand_dims(X_val_log, -1)+1.3)\nnp.save('data/Y_val_log.npy', Y_val_log.astype(np.int))\nnp.save('data/Y_val_log_all.npy', Y_val_log_all.astype(np.int))"
        }, 
        {
            "source": "## FFT\nFFTs were neccessary for inputs into our non-deep MLP and SVM model given that they only take 2D matrices as input. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def make_fft(file, file_dir = processed_audio_path):\n   \n\n    y, fs = librosa.load(file_dir + file, sr = 16000)\n    T = 1.0 / fs\n    N = y.shape[0]\n    yf = fft(y)\n    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n    vals = 2.0/N * np.abs(yf[0:N//2])  # FFT is symmetrical, so we take just the first half\n    # FFT is also complex, to we take just the real part (abs)\n    return vals.astype(np.float32)\n    "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def create_ffts(file_list = training_list):\n    X_array = np.zeros([len(file_list),8000])\n    Y_array = np.zeros([len(file_list)])    \n    for ind, file in enumerate(file_list):\n        if ind%2000 == 0:\n            print(ind, file)    \n        #try:\n        X_array[ind] = make_fft(file)\n       # except ValueError:\n        #    print(ind, file, ValueError)'''\n        Y_array[ind] = all_classes.index(file.rsplit('/')[0])\n        \n    return X_array, Y_array     "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_train_fft, Y_train_all_fft = create_ffts()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "np.save('data/X_train_fft.npy', X_train_fft)\nnp.save('data/Y_train_fft.npy', Y_train_fft.astype(np.int))\nnp.save('data/Y_train_all_fft.npy', Y_train_all_fft.astype(np.int))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_val_fft, Y_val_all_fft = create_ffts(file_list = validation_list)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Y_val_fft = np.where(Y_val_all_fft < 11, Y_val_all_fft, 11)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nnp.save('data/X_val_fft.npy', X_val_fft)\nnp.save('data/Y_val_fft.npy', Y_val_fft.astype(np.int))\nnp.save('data/Y_val_all_fft.npy', Y_val_all_fft.astype(np.int))"
        }, 
        {
            "source": "## Feature Transformation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Normalised sets were created for both spectrograms and ffts where each set was normalised agaisnst the mean and sd for every word individually. These also performed poorly and thus were not included in this final notebook", 
            "cell_type": "markdown", 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "source": "## Iterative Steps (Additional Feature for Evaluation Step)\n\nMFCC spectrograms were created to test if they improved accuracy in our network. However these again yielded poorer results as you'll see in the Evaluation stage.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 52, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def create_sets_mfcc(file_list = training_list):\n    X_array = np.zeros([len(file_list),32,13])\n    Y_array = np.zeros([len(file_list)])    \n    for ind, file in enumerate(file_list):\n        if ind%2000 == 0:\n            print(ind, file)  \n        try:    \n            X_array[ind] = make_mfccspec(file)\n        except ValueError:\n            print(ind, file, ValueError)\n        Y_array[ind] = all_classes.index(file.rsplit('/')[0])\n        \n    return X_array, Y_array "
        }, 
        {
            "execution_count": 51, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def make_mfccspec(file, file_dir = processed_audio_path, flip = False, ps = False, st = 4):\n    \"\"\"\n    create a mfcc spectrogram using a pre-existing log-mel spec\n    \n    Args:\n        file (str): filename\n        file_dir (str): directory path\n        flip (bool): reverse time axis\n        ps (bool): pitch shift\n        st (int): half-note steps for pitch shift\n    Returns:\n        np.array with shape (122,85) (time, freq)\n    \"\"\"\n    sig, rate = sf.read(file_dir + file)\n    if len(sig) < 16000: # pad shorter than 1 sec audio with ramp to zero\n        sig = np.pad(sig, (0,16000-len(sig)), 'linear_ramp')\n    \n    mfcc = librosa.feature.mfcc(sig, sr = 16000, n_mfcc=13).T\n\n    # Let's pad on the first and second deltas while we're at it\n    delta2_mfcc = librosa.feature.delta(mfcc, order=2, mode='nearest')\n\n    return delta2_mfcc.astype(np.float32)"
        }, 
        {
            "execution_count": 53, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0 no/8830e17f_nohash_1.wav\n2000 no/28497c5b_nohash_1.wav\n4000 eight/6af4aa07_nohash_1.wav\n6000 one/4c4d2526_nohash_1.wav\n8000 up/0137b3f4_nohash_4.wav\n10000 happy/48a8a69d_nohash_0.wav\n12000 off/f8f60f59_nohash_4.wav\n14000 on/1a673010_nohash_0.wav\n16000 cat/8f811bbc_nohash_0.wav\n18000 stop/4c77947d_nohash_1.wav\n20000 three/3389305e_nohash_0.wav\n22000 yes/190821dc_nohash_2.wav\n24000 yes/61d3e51e_nohash_0.wav\n26000 nine/3bc21161_nohash_2.wav\n28000 down/23abe1c9_nohash_0.wav\n30000 two/14587ff0_nohash_0.wav\n32000 marvin/a84dee7b_nohash_1.wav\n34000 dog/d103dd6e_nohash_0.wav\n36000 bird/e0344f60_nohash_1.wav\n38000 bed/f174517e_nohash_1.wav\n40000 left/c71e3acc_nohash_0.wav\n42000 right/2da58b32_nohash_2.wav\n44000 right/652b3da7_nohash_0.wav\n46000 seven/e53139ad_nohash_3.wav\n48000 six/f4386675_nohash_1.wav\n50000 five/caf9fceb_nohash_0.wav\n52000 four/66276b0e_nohash_1.wav\n54000 four/66aa0f29_nohash_0.wav\n56000 zero/aba19127_nohash_1.wav\n58000 go/f19c1390_nohash_1.wav\n"
                }
            ], 
            "source": "X_train_mfcc, Y_train_mfcc_all = create_sets_mfcc()\n"
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Y_train_mfcc = np.where(Y_train_mfcc_all < 11, Y_train_mfcc_all, 11)"
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "np.save('data/X_train_mfcc.npy', np.expand_dims(X_train_mfcc, -1)+1.3)\nnp.save('data/Y_train_mfcc.npy', Y_train_mfcc.astype(np.int))\nnp.save('data/Y_train_mfcc_all.npy', Y_train_mfcc_all.astype(np.int))"
        }, 
        {
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0 bed/f84762e5_nohash_0.wav\n2000 three/a6d586b7_nohash_4.wav\n4000 bird/8910e5ef_nohash_0.wav\n6000 go/471a0925_nohash_4.wav\n"
                }
            ], 
            "source": "X_val_mfcc, Y_val_mfcc_all = create_sets_mfcc(file_list = validation_list)\n"
        }, 
        {
            "execution_count": 60, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Y_val_mfcc = np.where(Y_val_mfcc_all < 11, Y_val_mfcc_all, 11)"
        }, 
        {
            "execution_count": 61, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "np.save('data/X_val_mfcc.npy', np.expand_dims(X_val_mfcc, -1)+1.3)\nnp.save('data/Y_val_mfcc.npy', Y_val_mfcc.astype(np.int))\nnp.save('data/Y_val_mfcc_all.npy', Y_val_mfcc_all.astype(np.int))"
        }, 
        {
            "source": "Save all training and validation sets to IBM COS.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}