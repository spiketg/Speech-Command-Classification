{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Model Definition", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "The following code defines the models to be evaluated (according to the best configurations found during testing), these were then saved to IBM COS for loading later.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nfrom keras.utils import to_categorical, plot_model\nfrom keras import backend as K\nfrom time import time\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn import svm, preprocessing\nfrom sklearn.metrics import classification_report, confusion_matrix  \nimport pickle"
        }, 
        {
            "source": "## SVM Definition", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "C=10\n\n#svm_mod = svm.LinearSVC(C=C, max_iter=10)\nsvm_mod = OneVsOneClassifier(svm.LinearSVC(random_state=0, C=C, max_iter=10))"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "filename = './models/svm.sav'\npickle.dump(svm_mod, open(filename, 'wb'))"
        }, 
        {
            "source": "## Neural Network MLP Definition", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nn_mod = MLPClassifier(hidden_layer_sizes = (50,))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "filename = './models/Neural Network.sav'\npickle.dump(nn_mod, open(filename, 'wb'))"
        }, 
        {
            "source": "## Deep CNN (RESNET) Definition", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.layers import *\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.merge import Add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.utils import plot_model\n\n\n#%%\nclass ResNet():\n    \"\"\"\n    Usage: \n        sr = ResNet([4,8,16], input_size=(50,50,1), output_size=12)\n        sr.build()\n        followed by sr.m.compile(loss='categorical_crossentropy', \n                                 optimizer='adadelta', metrics=[\"accuracy\"])\n        save plotted model with: \n            keras.utils.plot_model(sr.m, to_file = '<location>.png', \n                                   show_shapes=True)\n    \"\"\"\n    def __init__(self,\n                 filters_list=[], \n                 input_size=None, \n                 output_size=None,\n                 initializer='glorot_uniform'):\n        self.filters_list = filters_list\n        self.input_size = input_size\n        self.output_size = output_size\n        self.initializer = initializer\n        self.m = None        \n    \n    def _block(self, filters, inp):\n        \"\"\" one residual block in a ResNet\n        \n        Args:\n            filters (int): number of convolutional filters\n            inp (tf.tensor): output from previous layer\n            \n        Returns:\n            tf.tensor: output of residual block\n        \"\"\"\n        layer_1 = BatchNormalization()(inp)\n        act_1 = Activation('relu')(layer_1)\n        conv_1 = Conv2D(filters, (3,3), \n                        padding = 'same', \n                        kernel_initializer = self.initializer)(act_1)\n        layer_2 = BatchNormalization()(conv_1)\n        act_2 = Activation('relu')(layer_2)\n        conv_2 = Conv2D(filters, (3,3), \n                        padding = 'same', \n                        kernel_initializer = self.initializer)(act_2)\n        return(conv_2)\n\n    def build(self):\n        \"\"\"\n        Returns:\n            keras.engine.training.Model\n        \"\"\"\n        i = Input(shape = self.input_size, name = 'input')\n        x = Conv2D(self.filters_list[0], (3,3), \n                   padding = 'same', \n                   kernel_initializer = self.initializer)(i)\n        x = MaxPooling2D(padding = 'same')(x)        \n        x = Add()([self._block(self.filters_list[0], x),x])\n        x = Add()([self._block(self.filters_list[0], x),x])\n        x = Add()([self._block(self.filters_list[0], x),x])\n        if len(self.filters_list) > 1:\n            for filt in self.filters_list[1:]:\n                x = Conv2D(filt, (3,3),\n                           strides = (2,2),\n                           padding = 'same',\n                           activation = 'relu',\n                           kernel_initializer = self.initializer)(x)\n                x = Add()([self._block(filt, x),x])\n                x = Add()([self._block(filt, x),x])\n                x = Add()([self._block(filt, x),x])\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(self.output_size, activation = 'softmax')(x)\n        \n        self.m = Model(i,x)\n        return self.m"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_train = np.load('./data/X_train.npy')\nfilters_list = [8,16,32]\noutput_size = 12\ninput_size = X_train.shape[1:]"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sr = ResNet(filters_list, input_size, output_size)\nsr.build()\nsr.m.compile(loss='categorical_crossentropy', \n             optimizer='adadelta', \n             metrics=['accuracy'])"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "filename = './models/RESNET.h5'\nsr.m.save(filename)"
        }, 
        {
            "source": "save all models to IBM cos", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from ibm_botocore.client import Config\nimport ibm_boto3\ncos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials_1['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials_1['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials_1['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials_1['ENDPOINT'])"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\ncos.upload_file(Filename='models/RESNET.h5',Bucket=credentials_1['BUCKET'],Key='RESNET.h5')\ncos.upload_file(Filename='models/Neural Network.sav',Bucket=credentials_1['BUCKET'],Key='Neural Network.sav')\ncos.upload_file(Filename='models/svm.sav',Bucket=credentials_1['BUCKET'],Key='svm.sav')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.14", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}